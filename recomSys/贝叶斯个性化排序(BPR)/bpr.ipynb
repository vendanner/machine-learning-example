{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "项目描述：随机给两个电影，判断用户对电影的喜好\n",
    "贝叶斯个性化排序是将排序问题转化为序列对：(i>j>z) ==> (i,j),(j,z),\n",
    "训练这两个数据集就可以得到正确的排序\n",
    "原理的参考这篇博文：https://www.cnblogs.com/pinard/p/9163481.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.下载movielens 1M数据，根据打分构造样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 6040 6040 3952\n",
      "[(914, 3), (3408, 4), (2355, 5), (1197, 3), (1287, 5), (2804, 5), (594, 4), (919, 4), (595, 5), (938, 4), (2398, 4), (2918, 4), (1035, 5), (2791, 4), (2687, 3), (2018, 4), (3105, 5), (2797, 4), (2321, 3), (720, 3), (1270, 5), (527, 5), (2340, 3), (48, 5), (1097, 4), (1721, 4), (1545, 4), (745, 3), (2294, 4), (3186, 4), (1566, 4), (588, 4), (1907, 4), (783, 4), (1836, 5), (1022, 5), (2762, 4), (150, 5), (1, 5), (1961, 5), (1962, 4), (2692, 4), (260, 4), (1028, 5), (1029, 5), (1207, 4), (2028, 5), (531, 4), (3114, 4), (608, 4), (1246, 4)]\n",
      "[(1193, 5), (661, 3)]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(data_path):\n",
    "    #\n",
    "    train_data = defaultdict(list)\n",
    "    test_data = defaultdict(list)\n",
    "    watch_list = defaultdict(set)\n",
    "    max_uid = -1\n",
    "    max_item = -1\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            u, i, r, t = map(int,line.strip().split(\"::\"))\n",
    "            watch_list[u].add(i)\n",
    "            # 每个用户的评分都存在dict\n",
    "            # 划分训练集和测试集，测试集为用户最早的两条打分记录\n",
    "            if len(test_data[u]) == 0:\n",
    "                test_data[u].append((i,r))\n",
    "            elif len(test_data[u]) == 1 and test_data[u][0][1] != r:\n",
    "                test_data[u].append((i,r))\n",
    "            else:\n",
    "                train_data[u].append((i,r))\n",
    "            if u > max_uid:\n",
    "                max_uid = u\n",
    "            if i > max_item:\n",
    "                max_item = i\n",
    "    return train_data,test_data,watch_list,max_uid,max_item\n",
    "\n",
    "train_data, test_data, watch_list, user_count, item_count = load_data('ml-1m/ratings.dat')\n",
    "print(len(train_data), len(test_data), user_count, item_count)\n",
    "print(train_data[1])\n",
    "print(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.随机采样，生成batch训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5400 3623 2890]\n",
      " [  74  858   34]\n",
      " [2925  886 3895]\n",
      " [5577 3685 3668]\n",
      " [2920  902 1544]]\n"
     ]
    }
   ],
   "source": [
    "def generate_train_batch(rating_data, batch_size=256):\n",
    "    t = []\n",
    "    for b in range(batch_size):\n",
    "        u = random.sample(rating_data.keys(), 1)[0]\n",
    "        i,r1 = random.sample(rating_data[u], 1)[0]\n",
    "        j,r2 = random.sample(rating_data[u], 1)[0] \n",
    "        # 直到选到一个打分不同的pair\n",
    "        while r1 == r2:\n",
    "            u = random.sample(rating_data.keys(), 1)[0]\n",
    "            i,r1 = random.sample(rating_data[u], 1)[0]\n",
    "            j,r2 = random.sample(rating_data[u], 1)[0]\n",
    "        #i>j\n",
    "        if r1 > r2:\n",
    "            t.append([u, i, j])\n",
    "        else:\n",
    "            t.append([u, j, i])\n",
    "    return numpy.asarray(t)\n",
    "batch_data = generate_train_batch(train_data)\n",
    "# [user,moive1,moive2] 但是rat(moive1) > rat(moive2)\n",
    "print(batch_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1 1193  661]\n",
      " [   2 1357 3068]\n",
      " [   3 3421 1641]\n",
      " [   4 3468 1210]\n",
      " [   5 1175 2987]]\n"
     ]
    }
   ],
   "source": [
    "def generate_test_batch(rating_data):\n",
    "    t = []\n",
    "    for u in rating_data:\n",
    "        i,r1 = rating_data[u][0]\n",
    "        j,r2 = rating_data[u][1]\n",
    "        if r1 > r2:\n",
    "            t.append([u, i, j])\n",
    "        else:\n",
    "            t.append([u, j, i])\n",
    "    return numpy.asarray(t)\n",
    "batch_data = generate_test_batch(test_data)\n",
    "# [user,moive1,moive2] 但是rat(moive1) > rat(moive2)\n",
    "print(batch_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulation_rate = 0.01\n",
    "bias_reg = 0.01\n",
    "\n",
    "def bpr_mf(user_count, item_count, hidden_dim):\n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # Uv*Iv + Ub +Ib\n",
    "        # 定义 用户、物品、偏置的 Embedding 矩阵 既 bpr 中的 W 和H\n",
    "        user_vec = tf.get_variable(\"user_vec\", [user_count+1, hidden_dim], \n",
    "                            initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        item_vec = tf.get_variable(\"item_vec\", [item_count+1, hidden_dim], \n",
    "                                initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        item_bias = tf.get_variable(\"item_bias\", [item_count+1, 1], initializer=tf.random_normal_initializer(0, 0.1))    #item bias\n",
    "        \n",
    "        # 取出 Embeding 后的用户向量\n",
    "        u_vec = tf.nn.embedding_lookup(user_vec, u)\n",
    "        i_vec = tf.nn.embedding_lookup(item_vec, i)\n",
    "        j_vec = tf.nn.embedding_lookup(item_vec, j)  \n",
    "        #\n",
    "        i_bias = tf.nn.embedding_lookup(item_bias, i)       \n",
    "        j_bias = tf.nn.embedding_lookup(item_bias, j)  \n",
    "        \n",
    "        # 计算各自商品的得分\n",
    "        xui = i_bias + tf.reduce_sum(tf.multiply(u_vec, i_vec), 1, keep_dims=True)\n",
    "        xuj = j_bias + tf.reduce_sum(tf.multiply(u_vec, j_vec), 1, keep_dims=True)\n",
    "        xuij = xui-xuj\n",
    "        #i+  j+ i>j\n",
    "        auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "        # norm\n",
    "        l2_norm = tf.add_n([\n",
    "              regulation_rate * tf.reduce_sum(tf.multiply(u_vec, u_vec)),\n",
    "              regulation_rate * tf.reduce_sum(tf.multiply(i_vec, i_vec)),\n",
    "              regulation_rate * tf.reduce_sum(tf.multiply(j_vec, j_vec)),\n",
    "              bias_reg * tf.reduce_sum(tf.multiply(i_bias, i_bias)),\n",
    "              bias_reg * tf.reduce_sum(tf.multiply(j_bias, j_bias)),\n",
    "          ]) \n",
    "        #auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "        # 目标函数 = tf.reduce_mean(tf.log(tf.sigmoid(xuij))) -  l2_norm   最大化\n",
    "        # 那么求 l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij))) 最小值即可\n",
    "        # 将最大化问题转成最最小值问题\n",
    "        bprloss =  l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(xuij))) \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # minimize 最大微分 求梯度\n",
    "        train_op =  tf.train.AdamOptimizer().minimize(bprloss, global_step=global_step)  \n",
    "    return u, i, j,auc, bprloss, train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.batch训练评估效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  100\n",
      "train_loss:  2.6148086261749266\n",
      "train_auc:  0.504765625\n",
      "test_loss:  34.37083\n",
      "test_auc:  0.5084437\n",
      "epoch:  200\n",
      "train_loss:  1.8188515996932983\n",
      "train_auc:  0.5390625\n",
      "test_loss:  22.701008\n",
      "test_auc:  0.5294702\n",
      "epoch:  300\n",
      "train_loss:  1.407367649078369\n",
      "train_auc:  0.568671875\n",
      "test_loss:  15.972724\n",
      "test_auc:  0.56009936\n",
      "epoch:  400\n",
      "train_loss:  1.171027740240097\n",
      "train_auc:  0.59921875\n",
      "test_loss:  11.638198\n",
      "test_auc:  0.5948675\n",
      "epoch:  500\n",
      "train_loss:  1.0230027079582213\n",
      "train_auc:  0.628671875\n",
      "test_loss:  8.8713255\n",
      "test_auc:  0.61672187\n",
      "epoch:  600\n",
      "train_loss:  0.9290515553951263\n",
      "train_auc:  0.6473046875\n",
      "test_loss:  6.9396696\n",
      "test_auc:  0.63195366\n",
      "epoch:  700\n",
      "train_loss:  0.8616610205173493\n",
      "train_auc:  0.655078125\n",
      "test_loss:  5.5858364\n",
      "test_auc:  0.63327813\n",
      "epoch:  800\n",
      "train_loss:  0.8148200434446334\n",
      "train_auc:  0.663203125\n",
      "test_loss:  4.5690975\n",
      "test_auc:  0.64139074\n",
      "epoch:  900\n",
      "train_loss:  0.7818043822050095\n",
      "train_auc:  0.669921875\n",
      "test_loss:  3.8697934\n",
      "test_auc:  0.6428808\n",
      "epoch:  1000\n",
      "train_loss:  0.7584762853384018\n",
      "train_auc:  0.6717578125\n",
      "test_loss:  3.3148866\n",
      "test_auc:  0.64221853\n",
      "epoch:  1100\n",
      "train_loss:  0.7404243969917297\n",
      "train_auc:  0.6773046875\n",
      "test_loss:  2.9340563\n",
      "test_auc:  0.64983445\n",
      "epoch:  1200\n",
      "train_loss:  0.7278419589996338\n",
      "train_auc:  0.6805078125\n",
      "test_loss:  2.641903\n",
      "test_auc:  0.65281457\n",
      "epoch:  1300\n",
      "train_loss:  0.7180709964036942\n",
      "train_auc:  0.69015625\n",
      "test_loss:  2.3483143\n",
      "test_auc:  0.65298015\n",
      "epoch:  1400\n",
      "train_loss:  0.7110975486040115\n",
      "train_auc:  0.683515625\n",
      "test_loss:  2.1417527\n",
      "test_auc:  0.65331125\n",
      "epoch:  1500\n",
      "train_loss:  0.7060055303573608\n",
      "train_auc:  0.6878125\n",
      "test_loss:  2.0091596\n",
      "test_auc:  0.65827817\n",
      "epoch:  1600\n",
      "train_loss:  0.7019373565912247\n",
      "train_auc:  0.6858984375\n",
      "test_loss:  1.874265\n",
      "test_auc:  0.6574503\n",
      "epoch:  1700\n",
      "train_loss:  0.6990756052732467\n",
      "train_auc:  0.6814453125\n",
      "test_loss:  1.7714007\n",
      "test_auc:  0.6597682\n",
      "epoch:  1800\n",
      "train_loss:  0.6964034885168076\n",
      "train_auc:  0.689765625\n",
      "test_loss:  1.7073023\n",
      "test_auc:  0.6629139\n",
      "epoch:  1900\n",
      "train_loss:  0.6951342976093292\n",
      "train_auc:  0.687421875\n",
      "test_loss:  1.6657605\n",
      "test_auc:  0.6640729\n",
      "epoch:  2000\n",
      "train_loss:  0.6937405931949615\n",
      "train_auc:  0.6888671875\n",
      "test_loss:  1.6118906\n",
      "test_auc:  0.6673841\n",
      "epoch:  2100\n",
      "train_loss:  0.6931752783060073\n",
      "train_auc:  0.6844140625\n",
      "test_loss:  1.5603212\n",
      "test_auc:  0.6637417\n",
      "epoch:  2200\n",
      "train_loss:  0.69203184902668\n",
      "train_auc:  0.6870703125\n",
      "test_loss:  1.5173705\n",
      "test_auc:  0.6508278\n",
      "epoch:  2300\n",
      "train_loss:  0.6914584577083588\n",
      "train_auc:  0.6918359375\n",
      "test_loss:  1.5037265\n",
      "test_auc:  0.66241723\n",
      "epoch:  2400\n",
      "train_loss:  0.6910504043102265\n",
      "train_auc:  0.6936328125\n",
      "test_loss:  1.4823194\n",
      "test_auc:  0.65877485\n",
      "epoch:  2500\n",
      "train_loss:  0.6905530571937561\n",
      "train_auc:  0.688125\n",
      "test_loss:  1.4731641\n",
      "test_auc:  0.65827817\n",
      "epoch:  2600\n",
      "train_loss:  0.6905132901668548\n",
      "train_auc:  0.6893359375\n",
      "test_loss:  1.4670506\n",
      "test_auc:  0.6551325\n",
      "epoch:  2700\n",
      "train_loss:  0.6903182119131088\n",
      "train_auc:  0.685234375\n",
      "test_loss:  1.4630831\n",
      "test_auc:  0.64470196\n",
      "epoch:  2800\n",
      "train_loss:  0.6900896650552749\n",
      "train_auc:  0.6901953125\n",
      "test_loss:  1.450741\n",
      "test_auc:  0.65993375\n",
      "epoch:  2900\n",
      "train_loss:  0.6901553970575333\n",
      "train_auc:  0.6853515625\n",
      "test_loss:  1.4221065\n",
      "test_auc:  0.65927154\n",
      "epoch:  3000\n",
      "train_loss:  0.6900649541616439\n",
      "train_auc:  0.6841015625\n",
      "test_loss:  1.4118679\n",
      "test_auc:  0.6486755\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    u, i, j, auc, bprloss, train_op = bpr_mf(user_count, item_count, 32)\n",
    "    tf.global_variables_initializer().run()\n",
    "    test_batch_data = generate_test_batch(test_data)\n",
    "    _batch_bprloss = 0\n",
    "    _batch_auc = 0\n",
    "    for epoch in range(1, 2001):\n",
    "        #\n",
    "        batch_data = generate_train_batch(train_data)\n",
    "        _auc,_bprloss, _train_op = session.run([auc, bprloss, train_op], \n",
    "                                feed_dict={u:batch_data[:,0], i:batch_data[:,1], j:batch_data[:,2]})\n",
    "        _batch_bprloss += _bprloss\n",
    "        _batch_auc += _auc\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print (\"epoch: \", epoch)\n",
    "            print (\"train_loss: \", _batch_bprloss / 100)\n",
    "            print (\"train_auc: \", _batch_auc / 100)\n",
    "            _batch_bprloss = 0\n",
    "            _batch_auc = 0\n",
    "            #\n",
    "            _auc, _bprloss = session.run([auc, bprloss],\n",
    "                                    feed_dict={u:test_batch_data[:,0], i:test_batch_data[:,1], j:test_batch_data[:,2]}\n",
    "                                )\n",
    "            \n",
    "            print(\"test_loss: \",_bprloss)\n",
    "            print(\"test_auc: \",_auc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
