{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的：利用用户的观点(review)，使用Word2Vec 训练出 word Embendding\n",
    "流程：\n",
    "   把每个 review 分句，然后清洗 ；这里直接将review 分解成句子为单位\n",
    "   对每个句子分成词，句子用词组表示['life', 's', 'like', 'that']\n",
    "   以句子为单位，去除无效词，做 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('..', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入无标签数据\n",
    "用于训练生成word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和第一个ipython notebook一样做数据的预处理\n",
    "稍稍有一点不一样的是，我们留了个候选，可以去除停用词，也可以不去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_stopwords = set(stopwords.words('english'))\n",
    "eng_stopwords = {}.fromkeys([ line.rstrip() for line in open('../stopwords.txt')])\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "#     分句\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1001 times\n",
      "method split_sentences called 2001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 3001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 4001 times\n",
      "method split_sentences called 5001 times\n",
      "method split_sentences called 6001 times\n",
      "method split_sentences called 7001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 8001 times\n",
      "method split_sentences called 9001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 10001 times\n",
      "method split_sentences called 11001 times\n",
      "method split_sentences called 12001 times\n",
      "method split_sentences called 13001 times\n",
      "method split_sentences called 14001 times\n",
      "method split_sentences called 15001 times\n",
      "method split_sentences called 16001 times\n",
      "method split_sentences called 17001 times\n",
      "method split_sentences called 18001 times\n",
      "method split_sentences called 19001 times\n",
      "method split_sentences called 20001 times\n",
      "method split_sentences called 21001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 22001 times\n",
      "method split_sentences called 23001 times\n",
      "method split_sentences called 24001 times\n",
      "method split_sentences called 25001 times\n",
      "method split_sentences called 26001 times\n",
      "method split_sentences called 27001 times\n",
      "method split_sentences called 28001 times\n",
      "method split_sentences called 29001 times\n",
      "method split_sentences called 30001 times\n",
      "method split_sentences called 31001 times\n",
      "method split_sentences called 32001 times\n",
      "method split_sentences called 33001 times\n",
      "method split_sentences called 34001 times\n",
      "method split_sentences called 35001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 36001 times\n",
      "method split_sentences called 37001 times\n",
      "method split_sentences called 38001 times\n",
      "method split_sentences called 39001 times\n",
      "method split_sentences called 40001 times\n",
      "method split_sentences called 41001 times\n",
      "method split_sentences called 42001 times\n",
      "method split_sentences called 43001 times\n",
      "method split_sentences called 44001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 45001 times\n",
      "method split_sentences called 46001 times\n",
      "method split_sentences called 47001 times\n",
      "method split_sentences called 48001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python36\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 49001 times\n",
      "Wall time: 8min 57s\n",
      "50000 reviews -> 537851 sentences\n"
     ]
    }
   ],
   "source": [
    "%time sentences = sum(df.review.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(df), len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw this film about 20 years ago and remember it as being particularly nasty. I believe it is based on a true incident: a young man breaks into a nurses' home and rapes, tortures and kills various women.<br /><br />It is in black and white but saves the colour for one shocking shot.<br /><br />At the end the film seems to be trying to make some political statement but it just comes across as confused and obscene.<br /><br />Avoid.\n",
      "['I saw this film about 20 years ago and remember it as being particularly nasty.', \"I believe it is based on a true incident: a young man breaks into a nurses' home and rapes, tortures and kills various women.<br /><br />It is in black and white but saves the colour for one shocking shot.<br /><br />At the end the film seems to be trying to make some political statement but it just comes across as confused and obscene.<br /><br />Avoid.\"]\n",
      "[['i', 'saw', 'this', 'film', 'about', 'years', 'ago', 'and', 'remember', 'it', 'as', 'being', 'particularly', 'nasty'], ['i', 'believe', 'it', 'is', 'based', 'on', 'a', 'true', 'incident', 'a', 'young', 'man', 'breaks', 'into', 'a', 'nurses', 'home', 'and', 'rapes', 'tortures', 'and', 'kills', 'various', 'women', 'it', 'is', 'in', 'black', 'and', 'white', 'but', 'saves', 'the', 'colour', 'for', 'one', 'shocking', 'shot', 'at', 'the', 'end', 'the', 'film', 'seems', 'to', 'be', 'trying', 'to', 'make', 'some', 'political', 'statement', 'but', 'it', 'just', 'comes', 'across', 'as', 'confused', 'and', 'obscene', 'avoid']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['life', 's', 'like', 'that']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理步骤展示\n",
    "print(df['review'][1])\n",
    "print(tokenizer.tokenize(df['review'][1].strip()))\n",
    "print([clean_text(s) for s in tokenizer.tokenize(df['review'][1].strip())])\n",
    "sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-02 11:22:01,126 : INFO : collecting all words and their counts\n",
      "2019-04-02 11:22:01,127 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-02 11:22:01,194 : INFO : PROGRESS: at sentence #10000, processed 225072 words, keeping 17237 word types\n",
      "2019-04-02 11:22:01,260 : INFO : PROGRESS: at sentence #20000, processed 443536 words, keeping 24570 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-02 11:22:01,323 : INFO : PROGRESS: at sentence #30000, processed 666343 words, keeping 29785 word types\n",
      "2019-04-02 11:22:01,390 : INFO : PROGRESS: at sentence #40000, processed 886903 words, keeping 33939 word types\n",
      "2019-04-02 11:22:01,452 : INFO : PROGRESS: at sentence #50000, processed 1103863 words, keeping 37503 word types\n",
      "2019-04-02 11:22:01,519 : INFO : PROGRESS: at sentence #60000, processed 1327231 words, keeping 40738 word types\n",
      "2019-04-02 11:22:01,582 : INFO : PROGRESS: at sentence #70000, processed 1550828 words, keeping 43603 word types\n",
      "2019-04-02 11:22:01,652 : INFO : PROGRESS: at sentence #80000, processed 1772824 words, keeping 46155 word types\n",
      "2019-04-02 11:22:01,717 : INFO : PROGRESS: at sentence #90000, processed 1987492 words, keeping 48328 word types\n",
      "2019-04-02 11:22:01,793 : INFO : PROGRESS: at sentence #100000, processed 2210772 words, keeping 50551 word types\n",
      "2019-04-02 11:22:01,867 : INFO : PROGRESS: at sentence #110000, processed 2435500 words, keeping 52762 word types\n",
      "2019-04-02 11:22:01,935 : INFO : PROGRESS: at sentence #120000, processed 2658453 words, keeping 54893 word types\n",
      "2019-04-02 11:22:02,001 : INFO : PROGRESS: at sentence #130000, processed 2877966 words, keeping 56598 word types\n",
      "2019-04-02 11:22:02,067 : INFO : PROGRESS: at sentence #140000, processed 3098239 words, keeping 58352 word types\n",
      "2019-04-02 11:22:02,140 : INFO : PROGRESS: at sentence #150000, processed 3315374 words, keeping 60013 word types\n",
      "2019-04-02 11:22:02,212 : INFO : PROGRESS: at sentence #160000, processed 3536043 words, keeping 61691 word types\n",
      "2019-04-02 11:22:02,275 : INFO : PROGRESS: at sentence #170000, processed 3758389 words, keeping 63292 word types\n",
      "2019-04-02 11:22:02,343 : INFO : PROGRESS: at sentence #180000, processed 3979419 words, keeping 64846 word types\n",
      "2019-04-02 11:22:02,411 : INFO : PROGRESS: at sentence #190000, processed 4203552 words, keeping 66403 word types\n",
      "2019-04-02 11:22:02,479 : INFO : PROGRESS: at sentence #200000, processed 4429487 words, keeping 67924 word types\n",
      "2019-04-02 11:22:02,548 : INFO : PROGRESS: at sentence #210000, processed 4652926 words, keeping 69248 word types\n",
      "2019-04-02 11:22:02,609 : INFO : PROGRESS: at sentence #220000, processed 4870841 words, keeping 70567 word types\n",
      "2019-04-02 11:22:02,674 : INFO : PROGRESS: at sentence #230000, processed 5093110 words, keeping 71912 word types\n",
      "2019-04-02 11:22:02,743 : INFO : PROGRESS: at sentence #240000, processed 5311441 words, keeping 73234 word types\n",
      "2019-04-02 11:22:02,810 : INFO : PROGRESS: at sentence #250000, processed 5532201 words, keeping 74486 word types\n",
      "2019-04-02 11:22:02,879 : INFO : PROGRESS: at sentence #260000, processed 5751635 words, keeping 75693 word types\n",
      "2019-04-02 11:22:02,947 : INFO : PROGRESS: at sentence #270000, processed 5973499 words, keeping 76829 word types\n",
      "2019-04-02 11:22:03,015 : INFO : PROGRESS: at sentence #280000, processed 6191006 words, keeping 77953 word types\n",
      "2019-04-02 11:22:03,084 : INFO : PROGRESS: at sentence #290000, processed 6415986 words, keeping 79135 word types\n",
      "2019-04-02 11:22:03,153 : INFO : PROGRESS: at sentence #300000, processed 6634732 words, keeping 80229 word types\n",
      "2019-04-02 11:22:03,225 : INFO : PROGRESS: at sentence #310000, processed 6857611 words, keeping 81308 word types\n",
      "2019-04-02 11:22:03,300 : INFO : PROGRESS: at sentence #320000, processed 7077129 words, keeping 82421 word types\n",
      "2019-04-02 11:22:03,367 : INFO : PROGRESS: at sentence #330000, processed 7298672 words, keeping 83510 word types\n",
      "2019-04-02 11:22:03,432 : INFO : PROGRESS: at sentence #340000, processed 7516307 words, keeping 84446 word types\n",
      "2019-04-02 11:22:03,495 : INFO : PROGRESS: at sentence #350000, processed 7735106 words, keeping 85567 word types\n",
      "2019-04-02 11:22:03,563 : INFO : PROGRESS: at sentence #360000, processed 7956259 words, keeping 86545 word types\n",
      "2019-04-02 11:22:03,630 : INFO : PROGRESS: at sentence #370000, processed 8177579 words, keeping 87490 word types\n",
      "2019-04-02 11:22:03,708 : INFO : PROGRESS: at sentence #380000, processed 8395555 words, keeping 88516 word types\n",
      "2019-04-02 11:22:03,788 : INFO : PROGRESS: at sentence #390000, processed 8616523 words, keeping 89501 word types\n",
      "2019-04-02 11:22:03,855 : INFO : PROGRESS: at sentence #400000, processed 8835621 words, keeping 90471 word types\n",
      "2019-04-02 11:22:03,914 : INFO : PROGRESS: at sentence #410000, processed 9055389 words, keeping 91345 word types\n",
      "2019-04-02 11:22:03,976 : INFO : PROGRESS: at sentence #420000, processed 9276301 words, keeping 92246 word types\n",
      "2019-04-02 11:22:04,045 : INFO : PROGRESS: at sentence #430000, processed 9494464 words, keeping 93177 word types\n",
      "2019-04-02 11:22:04,125 : INFO : PROGRESS: at sentence #440000, processed 9719317 words, keeping 94120 word types\n",
      "2019-04-02 11:22:04,197 : INFO : PROGRESS: at sentence #450000, processed 9936920 words, keeping 94981 word types\n",
      "2019-04-02 11:22:04,261 : INFO : PROGRESS: at sentence #460000, processed 10160058 words, keeping 95782 word types\n",
      "2019-04-02 11:22:04,338 : INFO : PROGRESS: at sentence #470000, processed 10380745 words, keeping 96638 word types\n",
      "2019-04-02 11:22:04,416 : INFO : PROGRESS: at sentence #480000, processed 10599177 words, keeping 97472 word types\n",
      "2019-04-02 11:22:04,488 : INFO : PROGRESS: at sentence #490000, processed 10816564 words, keeping 98280 word types\n",
      "2019-04-02 11:22:04,560 : INFO : PROGRESS: at sentence #500000, processed 11032180 words, keeping 99065 word types\n",
      "2019-04-02 11:22:04,636 : INFO : PROGRESS: at sentence #510000, processed 11254513 words, keeping 99931 word types\n",
      "2019-04-02 11:22:04,738 : INFO : PROGRESS: at sentence #520000, processed 11481362 words, keeping 100837 word types\n",
      "2019-04-02 11:22:04,835 : INFO : PROGRESS: at sentence #530000, processed 11704023 words, keeping 101619 word types\n",
      "2019-04-02 11:22:04,913 : INFO : collected 102305 word types from a corpus of 11877527 raw words and 537851 sentences\n",
      "2019-04-02 11:22:04,916 : INFO : Loading a fresh vocabulary\n",
      "2019-04-02 11:22:05,049 : INFO : effective_min_count=40 retains 13056 unique words (12% of original 102305, drops 89249)\n",
      "2019-04-02 11:22:05,052 : INFO : effective_min_count=40 leaves 11401023 word corpus (95% of original 11877527, drops 476504)\n",
      "2019-04-02 11:22:05,154 : INFO : deleting the raw counts dictionary of 102305 items\n",
      "2019-04-02 11:22:05,162 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-04-02 11:22:05,163 : INFO : downsampling leaves estimated 8394666 word corpus (73.6% of prior 11401023)\n",
      "2019-04-02 11:22:05,243 : INFO : estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2019-04-02 11:22:05,245 : INFO : resetting layer weights\n",
      "2019-04-02 11:22:05,596 : INFO : training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-02 11:22:06,626 : INFO : EPOCH 1 - PROGRESS: at 6.45% examples, 536667 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-02 11:22:07,642 : INFO : EPOCH 1 - PROGRESS: at 14.07% examples, 584221 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:08,644 : INFO : EPOCH 1 - PROGRESS: at 22.13% examples, 614390 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-02 11:22:09,656 : INFO : EPOCH 1 - PROGRESS: at 30.07% examples, 624802 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:10,676 : INFO : EPOCH 1 - PROGRESS: at 38.20% examples, 635413 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:11,677 : INFO : EPOCH 1 - PROGRESS: at 46.21% examples, 641002 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:12,680 : INFO : EPOCH 1 - PROGRESS: at 53.29% examples, 633863 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:13,690 : INFO : EPOCH 1 - PROGRESS: at 59.74% examples, 621829 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:14,695 : INFO : EPOCH 1 - PROGRESS: at 65.99% examples, 610509 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:15,696 : INFO : EPOCH 1 - PROGRESS: at 73.94% examples, 615743 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:16,697 : INFO : EPOCH 1 - PROGRESS: at 81.92% examples, 620623 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:17,702 : INFO : EPOCH 1 - PROGRESS: at 89.92% examples, 624426 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:18,705 : INFO : EPOCH 1 - PROGRESS: at 98.14% examples, 629428 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-02 11:22:18,901 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-02 11:22:18,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-02 11:22:18,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-02 11:22:18,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-02 11:22:18,917 : INFO : EPOCH - 1 : training on 11877527 raw words (8394428 effective words) took 13.3s, 630965 effective words/s\n",
      "2019-04-02 11:22:19,933 : INFO : EPOCH 2 - PROGRESS: at 8.04% examples, 674411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:20,937 : INFO : EPOCH 2 - PROGRESS: at 15.93% examples, 667820 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:21,947 : INFO : EPOCH 2 - PROGRESS: at 23.45% examples, 654323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:22,949 : INFO : EPOCH 2 - PROGRESS: at 31.38% examples, 656075 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:23,956 : INFO : EPOCH 2 - PROGRESS: at 38.86% examples, 650925 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:24,961 : INFO : EPOCH 2 - PROGRESS: at 46.81% examples, 652393 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:25,977 : INFO : EPOCH 2 - PROGRESS: at 53.94% examples, 643436 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:26,988 : INFO : EPOCH 2 - PROGRESS: at 60.34% examples, 629335 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:27,992 : INFO : EPOCH 2 - PROGRESS: at 66.16% examples, 613338 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:28,995 : INFO : EPOCH 2 - PROGRESS: at 72.40% examples, 604029 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:30,001 : INFO : EPOCH 2 - PROGRESS: at 78.33% examples, 593803 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:31,004 : INFO : EPOCH 2 - PROGRESS: at 85.09% examples, 591785 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:32,004 : INFO : EPOCH 2 - PROGRESS: at 92.32% examples, 592360 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-02 11:22:33,019 : INFO : EPOCH 2 - PROGRESS: at 97.75% examples, 582267 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:33,400 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-02 11:22:33,458 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-02 11:22:33,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-02 11:22:33,470 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-02 11:22:33,472 : INFO : EPOCH - 2 : training on 11877527 raw words (8394082 effective words) took 14.5s, 577142 effective words/s\n",
      "2019-04-02 11:22:34,501 : INFO : EPOCH 3 - PROGRESS: at 5.94% examples, 495662 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:35,503 : INFO : EPOCH 3 - PROGRESS: at 12.89% examples, 539867 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:36,509 : INFO : EPOCH 3 - PROGRESS: at 20.28% examples, 565700 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:37,515 : INFO : EPOCH 3 - PROGRESS: at 27.53% examples, 575002 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:38,537 : INFO : EPOCH 3 - PROGRESS: at 34.66% examples, 577319 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:39,547 : INFO : EPOCH 3 - PROGRESS: at 42.58% examples, 591761 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:40,551 : INFO : EPOCH 3 - PROGRESS: at 50.82% examples, 605575 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-02 11:22:41,556 : INFO : EPOCH 3 - PROGRESS: at 58.56% examples, 610672 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:42,570 : INFO : EPOCH 3 - PROGRESS: at 66.75% examples, 617904 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-02 11:22:43,579 : INFO : EPOCH 3 - PROGRESS: at 73.78% examples, 614203 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:44,581 : INFO : EPOCH 3 - PROGRESS: at 79.42% examples, 601289 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-02 11:22:45,582 : INFO : EPOCH 3 - PROGRESS: at 86.79% examples, 602817 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:46,587 : INFO : EPOCH 3 - PROGRESS: at 94.53% examples, 605655 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:47,283 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-02 11:22:47,299 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-02 11:22:47,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-02 11:22:47,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-02 11:22:47,313 : INFO : EPOCH - 3 : training on 11877527 raw words (8397325 effective words) took 13.8s, 607490 effective words/s\n",
      "2019-04-02 11:22:48,326 : INFO : EPOCH 4 - PROGRESS: at 7.21% examples, 606504 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:49,345 : INFO : EPOCH 4 - PROGRESS: at 13.90% examples, 580127 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:50,352 : INFO : EPOCH 4 - PROGRESS: at 20.88% examples, 580425 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:51,366 : INFO : EPOCH 4 - PROGRESS: at 28.54% examples, 593480 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-02 11:22:52,376 : INFO : EPOCH 4 - PROGRESS: at 35.73% examples, 594695 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:53,378 : INFO : EPOCH 4 - PROGRESS: at 43.33% examples, 602247 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:54,384 : INFO : EPOCH 4 - PROGRESS: at 50.74% examples, 604426 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:55,386 : INFO : EPOCH 4 - PROGRESS: at 58.21% examples, 607193 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:56,391 : INFO : EPOCH 4 - PROGRESS: at 65.90% examples, 610688 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:22:57,413 : INFO : EPOCH 4 - PROGRESS: at 73.43% examples, 611131 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:58,417 : INFO : EPOCH 4 - PROGRESS: at 80.59% examples, 609891 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:22:59,429 : INFO : EPOCH 4 - PROGRESS: at 88.21% examples, 611941 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:00,435 : INFO : EPOCH 4 - PROGRESS: at 95.84% examples, 613441 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:23:00,949 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-02 11:23:00,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-02 11:23:00,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-02 11:23:00,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-02 11:23:00,964 : INFO : EPOCH - 4 : training on 11877527 raw words (8394488 effective words) took 13.6s, 615469 effective words/s\n",
      "2019-04-02 11:23:01,987 : INFO : EPOCH 5 - PROGRESS: at 7.04% examples, 586020 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:02,995 : INFO : EPOCH 5 - PROGRESS: at 14.57% examples, 607712 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:04,006 : INFO : EPOCH 5 - PROGRESS: at 22.21% examples, 616826 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:05,020 : INFO : EPOCH 5 - PROGRESS: at 29.65% examples, 615602 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:23:06,027 : INFO : EPOCH 5 - PROGRESS: at 36.96% examples, 615697 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:07,042 : INFO : EPOCH 5 - PROGRESS: at 44.53% examples, 617416 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:08,048 : INFO : EPOCH 5 - PROGRESS: at 52.29% examples, 621206 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:09,049 : INFO : EPOCH 5 - PROGRESS: at 59.83% examples, 622878 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:10,061 : INFO : EPOCH 5 - PROGRESS: at 67.32% examples, 622654 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:11,070 : INFO : EPOCH 5 - PROGRESS: at 74.79% examples, 621956 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:12,081 : INFO : EPOCH 5 - PROGRESS: at 81.92% examples, 619309 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:13,086 : INFO : EPOCH 5 - PROGRESS: at 88.74% examples, 615154 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-02 11:23:14,087 : INFO : EPOCH 5 - PROGRESS: at 96.26% examples, 616088 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-02 11:23:14,576 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-02 11:23:14,587 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-02 11:23:14,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-02 11:23:14,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-02 11:23:14,600 : INFO : EPOCH - 5 : training on 11877527 raw words (8395117 effective words) took 13.6s, 616162 effective words/s\n",
      "2019-04-02 11:23:14,601 : INFO : training on a 59387635 raw words (41975440 effective words) took 69.0s, 608343 effective words/s\n",
      "2019-04-02 11:23:14,602 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-02 11:23:14,618 : INFO : saving Word2Vec object under ..\\models\\300features_40minwords_10context.model, separately None\n",
      "2019-04-02 11:23:14,619 : INFO : not storing attribute vectors_norm\n",
      "2019-04-02 11:23:14,620 : INFO : not storing attribute cum_table\n",
      "2019-04-02 11:23:15,448 : INFO : saved ..\\models\\300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看训练的词向量结果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n"
     ]
    }
   ],
   "source": [
    "# doesnt_match 选出下列集合中不同词义的词 \n",
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match('france england germany berlin'.split()))\n",
    "# model 已经学习到词与词之间的关系了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6256189346313477),\n",
       " ('lady', 0.5953349471092224),\n",
       " ('lad', 0.576863169670105),\n",
       " ('person', 0.5407935380935669),\n",
       " ('farmer', 0.5382746458053589),\n",
       " ('chap', 0.536788821220398),\n",
       " ('soldier', 0.5292650461196899),\n",
       " ('men', 0.5261573791503906),\n",
       " ('monk', 0.5237958431243896),\n",
       " ('guy', 0.5213091373443604)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选出与 man 最相似的词\n",
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6749982833862305),\n",
       " ('maid', 0.6223365068435669),\n",
       " ('bride', 0.6201028227806091),\n",
       " ('belle', 0.6200867891311646),\n",
       " ('temple', 0.6171057224273682),\n",
       " ('stripper', 0.608874499797821),\n",
       " ('catherine', 0.6072724461555481),\n",
       " ('eva', 0.6019693613052368),\n",
       " ('dancer', 0.594109833240509),\n",
       " ('sylvia', 0.5933606624603271)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7551683187484741),\n",
       " ('atrocious', 0.7340768575668335),\n",
       " ('horrible', 0.7315883040428162),\n",
       " ('dreadful', 0.7080680131912231),\n",
       " ('abysmal', 0.7010548114776611),\n",
       " ('horrendous', 0.6951696872711182),\n",
       " ('appalling', 0.691646933555603),\n",
       " ('horrid', 0.6708598136901855),\n",
       " ('amateurish', 0.6481891870498657),\n",
       " ('embarrassing', 0.6306308507919312)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
